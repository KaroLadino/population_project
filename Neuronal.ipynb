{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projecto Final ML\n",
    "#### Población \n",
    "Precidiendo la población en Colombia / Redes neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import perceptron\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv (\"country_population_col.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anio</th>\n",
       "      <th>Population</th>\n",
       "      <th>Fertility</th>\n",
       "      <th>Life_Expectancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960</td>\n",
       "      <td>16480383</td>\n",
       "      <td>6.807</td>\n",
       "      <td>56.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1961</td>\n",
       "      <td>16982315</td>\n",
       "      <td>6.801</td>\n",
       "      <td>57.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1962</td>\n",
       "      <td>17500171</td>\n",
       "      <td>6.779</td>\n",
       "      <td>57.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1963</td>\n",
       "      <td>18033550</td>\n",
       "      <td>6.736</td>\n",
       "      <td>58.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1964</td>\n",
       "      <td>18581974</td>\n",
       "      <td>6.668</td>\n",
       "      <td>58.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1965</td>\n",
       "      <td>19144223</td>\n",
       "      <td>6.567</td>\n",
       "      <td>59.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1966</td>\n",
       "      <td>19721462</td>\n",
       "      <td>6.425</td>\n",
       "      <td>59.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1967</td>\n",
       "      <td>20311371</td>\n",
       "      <td>6.246</td>\n",
       "      <td>59.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1968</td>\n",
       "      <td>20905059</td>\n",
       "      <td>6.034</td>\n",
       "      <td>60.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1969</td>\n",
       "      <td>21490945</td>\n",
       "      <td>5.797</td>\n",
       "      <td>60.574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1970</td>\n",
       "      <td>22061215</td>\n",
       "      <td>5.548</td>\n",
       "      <td>60.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1971</td>\n",
       "      <td>22611986</td>\n",
       "      <td>5.300</td>\n",
       "      <td>61.248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1972</td>\n",
       "      <td>23146803</td>\n",
       "      <td>5.067</td>\n",
       "      <td>61.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1973</td>\n",
       "      <td>23674504</td>\n",
       "      <td>4.857</td>\n",
       "      <td>61.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1974</td>\n",
       "      <td>24208021</td>\n",
       "      <td>4.675</td>\n",
       "      <td>62.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1975</td>\n",
       "      <td>24756973</td>\n",
       "      <td>4.523</td>\n",
       "      <td>62.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1976</td>\n",
       "      <td>25323406</td>\n",
       "      <td>4.396</td>\n",
       "      <td>63.271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1977</td>\n",
       "      <td>25905127</td>\n",
       "      <td>4.284</td>\n",
       "      <td>63.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1978</td>\n",
       "      <td>26502166</td>\n",
       "      <td>4.177</td>\n",
       "      <td>64.354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1979</td>\n",
       "      <td>27113512</td>\n",
       "      <td>4.071</td>\n",
       "      <td>64.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1980</td>\n",
       "      <td>27737900</td>\n",
       "      <td>3.965</td>\n",
       "      <td>65.506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1981</td>\n",
       "      <td>28375991</td>\n",
       "      <td>3.855</td>\n",
       "      <td>66.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1982</td>\n",
       "      <td>29027162</td>\n",
       "      <td>3.744</td>\n",
       "      <td>66.541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1983</td>\n",
       "      <td>29687094</td>\n",
       "      <td>3.634</td>\n",
       "      <td>66.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1984</td>\n",
       "      <td>30350086</td>\n",
       "      <td>3.526</td>\n",
       "      <td>67.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1985</td>\n",
       "      <td>31011688</td>\n",
       "      <td>3.420</td>\n",
       "      <td>67.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1986</td>\n",
       "      <td>31669776</td>\n",
       "      <td>3.320</td>\n",
       "      <td>67.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1987</td>\n",
       "      <td>32324325</td>\n",
       "      <td>3.227</td>\n",
       "      <td>67.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1988</td>\n",
       "      <td>32975535</td>\n",
       "      <td>3.142</td>\n",
       "      <td>68.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1989</td>\n",
       "      <td>33624444</td>\n",
       "      <td>3.065</td>\n",
       "      <td>68.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1990</td>\n",
       "      <td>34271565</td>\n",
       "      <td>2.994</td>\n",
       "      <td>68.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1991</td>\n",
       "      <td>34916766</td>\n",
       "      <td>2.927</td>\n",
       "      <td>68.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1992</td>\n",
       "      <td>35558682</td>\n",
       "      <td>2.860</td>\n",
       "      <td>68.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1993</td>\n",
       "      <td>36195168</td>\n",
       "      <td>2.794</td>\n",
       "      <td>68.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1994</td>\n",
       "      <td>36823537</td>\n",
       "      <td>2.727</td>\n",
       "      <td>69.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1995</td>\n",
       "      <td>37441977</td>\n",
       "      <td>2.660</td>\n",
       "      <td>69.428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1996</td>\n",
       "      <td>38049038</td>\n",
       "      <td>2.596</td>\n",
       "      <td>69.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1997</td>\n",
       "      <td>38645411</td>\n",
       "      <td>2.535</td>\n",
       "      <td>70.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1998</td>\n",
       "      <td>39234062</td>\n",
       "      <td>2.481</td>\n",
       "      <td>70.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1999</td>\n",
       "      <td>39819279</td>\n",
       "      <td>2.432</td>\n",
       "      <td>70.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2000</td>\n",
       "      <td>40403958</td>\n",
       "      <td>2.389</td>\n",
       "      <td>71.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2001</td>\n",
       "      <td>40988909</td>\n",
       "      <td>2.349</td>\n",
       "      <td>71.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2002</td>\n",
       "      <td>41572491</td>\n",
       "      <td>2.311</td>\n",
       "      <td>71.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2003</td>\n",
       "      <td>42152151</td>\n",
       "      <td>2.274</td>\n",
       "      <td>71.813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2004</td>\n",
       "      <td>42724163</td>\n",
       "      <td>2.236</td>\n",
       "      <td>72.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2005</td>\n",
       "      <td>43285634</td>\n",
       "      <td>2.197</td>\n",
       "      <td>72.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2006</td>\n",
       "      <td>43835722</td>\n",
       "      <td>2.157</td>\n",
       "      <td>72.532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2007</td>\n",
       "      <td>44374572</td>\n",
       "      <td>2.118</td>\n",
       "      <td>72.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2008</td>\n",
       "      <td>44901544</td>\n",
       "      <td>2.080</td>\n",
       "      <td>72.952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2009</td>\n",
       "      <td>45416181</td>\n",
       "      <td>2.044</td>\n",
       "      <td>73.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2010</td>\n",
       "      <td>45918097</td>\n",
       "      <td>2.010</td>\n",
       "      <td>73.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2011</td>\n",
       "      <td>46406646</td>\n",
       "      <td>1.978</td>\n",
       "      <td>73.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2012</td>\n",
       "      <td>46881475</td>\n",
       "      <td>1.948</td>\n",
       "      <td>73.673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2013</td>\n",
       "      <td>47342981</td>\n",
       "      <td>1.921</td>\n",
       "      <td>73.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2014</td>\n",
       "      <td>47791911</td>\n",
       "      <td>1.897</td>\n",
       "      <td>74.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2015</td>\n",
       "      <td>48228697</td>\n",
       "      <td>1.874</td>\n",
       "      <td>74.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2016</td>\n",
       "      <td>48653419</td>\n",
       "      <td>1.853</td>\n",
       "      <td>74.381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Anio  Population  Fertility  Life_Expectancy\n",
       "0   1960    16480383      6.807           56.752\n",
       "1   1961    16982315      6.801           57.283\n",
       "2   1962    17500171      6.779           57.775\n",
       "3   1963    18033550      6.736           58.238\n",
       "4   1964    18581974      6.668           58.678\n",
       "5   1965    19144223      6.567           59.100\n",
       "6   1966    19721462      6.425           59.500\n",
       "7   1967    20311371      6.246           59.877\n",
       "8   1968    20905059      6.034           60.233\n",
       "9   1969    21490945      5.797           60.574\n",
       "10  1970    22061215      5.548           60.910\n",
       "11  1971    22611986      5.300           61.248\n",
       "12  1972    23146803      5.067           61.598\n",
       "13  1973    23674504      4.857           61.966\n",
       "14  1974    24208021      4.675           62.360\n",
       "15  1975    24756973      4.523           62.791\n",
       "16  1976    25323406      4.396           63.271\n",
       "17  1977    25905127      4.284           63.796\n",
       "18  1978    26502166      4.177           64.354\n",
       "19  1979    27113512      4.071           64.932\n",
       "20  1980    27737900      3.965           65.506\n",
       "21  1981    28375991      3.855           66.049\n",
       "22  1982    29027162      3.744           66.541\n",
       "23  1983    29687094      3.634           66.965\n",
       "24  1984    30350086      3.526           67.315\n",
       "25  1985    31011688      3.420           67.588\n",
       "26  1986    31669776      3.320           67.789\n",
       "27  1987    32324325      3.227           67.938\n",
       "28  1988    32975535      3.142           68.060\n",
       "29  1989    33624444      3.065           68.171\n",
       "30  1990    34271565      2.994           68.292\n",
       "31  1991    34916766      2.927           68.443\n",
       "32  1992    35558682      2.860           68.630\n",
       "33  1993    36195168      2.794           68.857\n",
       "34  1994    36823537      2.727           69.125\n",
       "35  1995    37441977      2.660           69.428\n",
       "36  1996    38049038      2.596           69.756\n",
       "37  1997    38645411      2.535           70.090\n",
       "38  1998    39234062      2.481           70.416\n",
       "39  1999    39819279      2.432           70.727\n",
       "40  2000    40403958      2.389           71.019\n",
       "41  2001    40988909      2.349           71.293\n",
       "42  2002    41572491      2.311           71.556\n",
       "43  2003    42152151      2.274           71.813\n",
       "44  2004    42724163      2.236           72.062\n",
       "45  2005    43285634      2.197           72.303\n",
       "46  2006    43835722      2.157           72.532\n",
       "47  2007    44374572      2.118           72.749\n",
       "48  2008    44901544      2.080           72.952\n",
       "49  2009    45416181      2.044           73.143\n",
       "50  2010    45918097      2.010           73.325\n",
       "51  2011    46406646      1.978           73.500\n",
       "52  2012    46881475      1.948           73.673\n",
       "53  2013    47342981      1.921           73.847\n",
       "54  2014    47791911      1.897           74.022\n",
       "55  2015    48228697      1.874           74.200\n",
       "56  2016    48653419      1.853           74.381"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cristian\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1960. 1961. 1962. 1963. 1964. 1965. 1966. 1967. 1968. 1969. 1970. 1971.\n 1972. 1973. 1974. 1975. 1976. 1977. 1978. 1979. 1980. 1981. 1982. 1983.\n 1984. 1985. 1986. 1987. 1988. 1989. 1990. 1991. 1992. 1993. 1994. 1995.\n 1996. 1997. 1998. 1999. 2000. 2001. 2002. 2003. 2004. 2005. 2006. 2007.\n 2008. 2009. 2010. 2011. 2012. 2013. 2014. 2015. 2016.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-96137d923d20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Initialise and fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperceptron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPerceptron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.002\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    584\u001b[0m                          \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                          sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    574\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1960. 1961. 1962. 1963. 1964. 1965. 1966. 1967. 1968. 1969. 1970. 1971.\n 1972. 1973. 1974. 1975. 1976. 1977. 1978. 1979. 1980. 1981. 1982. 1983.\n 1984. 1985. 1986. 1987. 1988. 1989. 1990. 1991. 1992. 1993. 1994. 1995.\n 1996. 1997. 1998. 1999. 2000. 2001. 2002. 2003. 2004. 2005. 2006. 2007.\n 2008. 2009. 2010. 2011. 2012. 2013. 2014. 2015. 2016.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "#predictors = ['Anio']\n",
    "X = data['Anio']\n",
    "y = data['Fertility']\n",
    "X = array.\n",
    "# Initialise and fit model\n",
    "clf = perceptron.Perceptron(n_iter=100, verbose=0, random_state=2017, fit_intercept=True, eta0=0.002)\n",
    "model = clf.fit(X, y, sample_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973\n 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987\n 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001\n 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\n 2016].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-43a58eb494e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \"\"\"\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    298\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973\n 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987\n 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001\n 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\n 2016].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [16480383 16480383 16480383 27113512 27113512 27113512 27113512 27113512\n",
      " 27113512 27113512 27113512 27113512 27113512 27113512 27113512 27113512\n",
      " 27113512 27113512 27113512 27113512 27113512 27113512 27113512 27113512\n",
      " 27113512 27113512 27113512 27113512 27113512 27113512 27113512 27113512\n",
      " 27113512 27113512 27113512 27113512 27113512 27113512 48653419 48653419\n",
      " 48653419 48653419 48653419 48653419 48653419 48653419 44374572 44374572\n",
      " 44374572 44374572 44374572 44374572 44374572 44374572 44374572 47791911\n",
      " 47791911]\n"
     ]
    }
   ],
   "source": [
    "print (\"Prediction: \" + str(model.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 0     16480383\n",
      "1     16982315\n",
      "2     17500171\n",
      "3     18033550\n",
      "4     18581974\n",
      "5     19144223\n",
      "6     19721462\n",
      "7     20311371\n",
      "8     20905059\n",
      "9     21490945\n",
      "10    22061215\n",
      "11    22611986\n",
      "12    23146803\n",
      "13    23674504\n",
      "14    24208021\n",
      "15    24756973\n",
      "16    25323406\n",
      "17    25905127\n",
      "18    26502166\n",
      "19    27113512\n",
      "20    27737900\n",
      "21    28375991\n",
      "22    29027162\n",
      "23    29687094\n",
      "24    30350086\n",
      "25    31011688\n",
      "26    31669776\n",
      "27    32324325\n",
      "28    32975535\n",
      "29    33624444\n",
      "30    34271565\n",
      "31    34916766\n",
      "32    35558682\n",
      "33    36195168\n",
      "34    36823537\n",
      "35    37441977\n",
      "36    38049038\n",
      "37    38645411\n",
      "38    39234062\n",
      "39    39819279\n",
      "40    40403958\n",
      "41    40988909\n",
      "42    41572491\n",
      "43    42152151\n",
      "44    42724163\n",
      "45    43285634\n",
      "46    43835722\n",
      "47    44374572\n",
      "48    44901544\n",
      "49    45416181\n",
      "50    45918097\n",
      "51    46406646\n",
      "52    46881475\n",
      "53    47342981\n",
      "54    47791911\n",
      "55    48228697\n",
      "56    48653419\n",
      "Name: Population, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (\"Actual: \" + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
